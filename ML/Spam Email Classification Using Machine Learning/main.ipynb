{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Email Classification Using Machine Learning\n",
    "### Group Members:\n",
    "Simon Lindqvist: siln22@student.bth.se\n",
    "\n",
    "Abdalrahman Mohammed: abmm22@student.bth.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jinja2 as j2\n",
    "import scipy.stats as stats\n",
    "\n",
    "# --- Import models ---\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# --- K fold and cross val stuff ---\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# --- Other stuff ---\n",
    "import time\n",
    "\n",
    "# --- Load data ---\n",
    "spambase_data_df = pd.read_csv(\"spambase.data\", header=None)\n",
    "x_data = spambase_data_df.iloc[:, :-1] # All columns except the last\n",
    "y_data = spambase_data_df.iloc[:, -1] # Last column is the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run stratified ten-fold cross-validation tests for each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfold setup\n",
    "n_folds = 10\n",
    "strat_k_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stratified kfold on the data with naive bayes classifier\n",
    "naive_bayes_accuracy = []\n",
    "naive_bayes_f1 = []\n",
    "naive_bates_time = []\n",
    "\n",
    "for train_index, test_index in strat_k_fold.split(x_data, y_data):\n",
    "    x_train, x_test = x_data.iloc[train_index], x_data.iloc[test_index]\n",
    "    y_train, y_test = y_data.iloc[train_index], y_data.iloc[test_index]\n",
    "\n",
    "    naive_bayes = GaussianNB()\n",
    "\n",
    "    timer = time.time()\n",
    "\n",
    "    naive_bayes.fit(x_train, y_train)\n",
    "\n",
    "    delta_time = time.time() - timer\n",
    "\n",
    "    y_pred = naive_bayes.predict(x_test)\n",
    "\n",
    "    naive_bayes_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    naive_bayes_f1.append(f1_score(y_test, y_pred))\n",
    "    naive_bates_time.append(delta_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stratified kfold on the data with knn classifier\n",
    "knn_accuracy = []\n",
    "knn_f1 = []\n",
    "knn_time = []\n",
    "\n",
    "for train_index, test_index in strat_k_fold.split(x_data, y_data):\n",
    "    x_train, x_test = x_data.iloc[train_index], x_data.iloc[test_index]\n",
    "    y_train, y_test = y_data.iloc[train_index], y_data.iloc[test_index]\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "    timer = time.time()\n",
    "\n",
    "    knn.fit(x_train, y_train)\n",
    "\n",
    "    delta_time = time.time() - timer\n",
    "\n",
    "    y_pred = knn.predict(x_test)\n",
    "\n",
    "    knn_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    knn_f1.append(f1_score(y_test, y_pred))\n",
    "    knn_time.append(delta_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run stratified kfold on the data with decision tree classifier\n",
    "decision_tree_accuracy = []\n",
    "decision_tree_f1 = []\n",
    "decision_tree_time = []\n",
    "\n",
    "for train_index, test_index in strat_k_fold.split(x_data, y_data):\n",
    "    x_train, x_test = x_data.iloc[train_index], x_data.iloc[test_index]\n",
    "    y_train, y_test = y_data.iloc[train_index], y_data.iloc[test_index]\n",
    "\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "    timer = time.time()\n",
    "\n",
    "    decision_tree.fit(x_train, y_train)\n",
    "\n",
    "    delta_time = time.time() - timer\n",
    "\n",
    "    y_pred = decision_tree.predict(x_test)\n",
    "\n",
    "    decision_tree_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    decision_tree_f1.append(f1_score(y_test, y_pred))\n",
    "    decision_tree_time.append(delta_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present the results exactly as in example 12.4 in course literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy latex table:\n",
      "\\begin{tabular}{|l|c|c|c|}\n",
      "\\hline\n",
      "Fold & Naive Bayes & KNN & Decision Tree \\\\\n",
      "\\midrule\n",
      "1 & 0.826 & 0.807 & 0.909 \\\\\n",
      "2 & 0.841 & 0.787 & 0.889 \\\\\n",
      "3 & 0.826 & 0.793 & 0.917 \\\\\n",
      "4 & 0.811 & 0.830 & 0.911 \\\\\n",
      "5 & 0.846 & 0.813 & 0.911 \\\\\n",
      "6 & 0.826 & 0.848 & 0.926 \\\\\n",
      "7 & 0.839 & 0.822 & 0.924 \\\\\n",
      "8 & 0.780 & 0.800 & 0.915 \\\\\n",
      "9 & 0.780 & 0.796 & 0.893 \\\\\n",
      "10 & 0.826 & 0.787 & 0.911 \\\\\n",
      "avg & 0.820 & 0.808 & 0.911 \\\\\n",
      "std & 0.022 & 0.019 & 0.011 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "F1 latex table:\n",
      "\\begin{tabular}{|l|c|c|c|}\n",
      "\\hline\n",
      "Fold & Naive Bayes & KNN & Decision Tree \\\\\n",
      "\\midrule\n",
      "1 & 0.810 & 0.753 & 0.888 \\\\\n",
      "2 & 0.827 & 0.732 & 0.861 \\\\\n",
      "3 & 0.815 & 0.740 & 0.895 \\\\\n",
      "4 & 0.800 & 0.780 & 0.887 \\\\\n",
      "5 & 0.830 & 0.758 & 0.885 \\\\\n",
      "6 & 0.814 & 0.800 & 0.908 \\\\\n",
      "7 & 0.826 & 0.776 & 0.902 \\\\\n",
      "8 & 0.769 & 0.753 & 0.896 \\\\\n",
      "9 & 0.775 & 0.749 & 0.869 \\\\\n",
      "10 & 0.813 & 0.718 & 0.887 \\\\\n",
      "avg & 0.808 & 0.756 & 0.888 \\\\\n",
      "std & 0.020 & 0.023 & 0.014 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "Time latex table:\n",
      "\\begin{tabular}{|l|c|c|c|}\n",
      "\\hline\n",
      "Fold & Naive Bayes & KNN & Decision Tree \\\\\n",
      "\\midrule\n",
      "1 & 0.007 & 0.006 & 0.079 \\\\\n",
      "2 & 0.007 & 0.006 & 0.062 \\\\\n",
      "3 & 0.008 & 0.005 & 0.069 \\\\\n",
      "4 & 0.008 & 0.004 & 0.067 \\\\\n",
      "5 & 0.008 & 0.005 & 0.062 \\\\\n",
      "6 & 0.010 & 0.005 & 0.062 \\\\\n",
      "7 & 0.006 & 0.005 & 0.054 \\\\\n",
      "8 & 0.008 & 0.006 & 0.072 \\\\\n",
      "9 & 0.008 & 0.005 & 0.067 \\\\\n",
      "10 & 0.008 & 0.005 & 0.071 \\\\\n",
      "avg & 0.008 & 0.005 & 0.066 \\\\\n",
      "std & 0.001 & 0.001 & 0.007 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_model_accuracy = [naive_bayes_accuracy, knn_accuracy, decision_tree_accuracy]\n",
    "all_model_f1 = [naive_bayes_f1, knn_f1, decision_tree_f1]\n",
    "all_model_time = [naive_bates_time, knn_time, decision_tree_time]\n",
    "\n",
    "# --- For accuracy ---\n",
    "\n",
    "# Make new df for accuracy\n",
    "models_accuracy_df = pd.DataFrame(columns=[\"Fold\", \"Naive Bayes\", \"KNN\", \"Decision Tree\"])\n",
    "models_accuracy_df[\"Fold\"] = range(1, n_folds+1)\n",
    "models_accuracy_df[\"Naive Bayes\"] = naive_bayes_accuracy\n",
    "models_accuracy_df[\"KNN\"] = knn_accuracy\n",
    "models_accuracy_df[\"Decision Tree\"] = decision_tree_accuracy\n",
    "\n",
    "# Add 11th row in fold column as mean\n",
    "models_accuracy_df.loc[n_folds] = [\"avg\", np.mean(naive_bayes_accuracy), np.mean(knn_accuracy), np.mean(decision_tree_accuracy)]\n",
    "# Add 12th row in fold column as std\n",
    "models_accuracy_df.loc[n_folds+1] = [\"std\", np.std(naive_bayes_accuracy), np.std(knn_accuracy), np.std(decision_tree_accuracy)]\n",
    "\n",
    "# Make into latex table using the to_latex function\n",
    "models_accuracy_print_latex = models_accuracy_df.to_latex(index=False, header=True, float_format=\"%.3f\", column_format=\"|l|c|c|c|\", escape=False)\n",
    "\n",
    "# Adjust LaTeX to add horizontal lines and match desired style\n",
    "models_accuracy_print_latex = models_accuracy_print_latex.replace(\"\\\\toprule\", \"\\\\hline\")\n",
    "models_accuracy_print_latex = models_accuracy_print_latex.replace(\"\\\\bottomrule\", \"\\\\hline\")\n",
    "\n",
    "# --- For f1 ---\n",
    "\n",
    "# Make new df for f1\n",
    "models_f1_df = pd.DataFrame(columns=[\"Fold\", \"Naive Bayes\", \"KNN\", \"Decision Tree\"])\n",
    "models_f1_df[\"Fold\"] = range(1, n_folds+1)\n",
    "models_f1_df[\"Naive Bayes\"] = naive_bayes_f1\n",
    "models_f1_df[\"KNN\"] = knn_f1\n",
    "models_f1_df[\"Decision Tree\"] = decision_tree_f1\n",
    "\n",
    "# Add 11th row in fold column as mean\n",
    "models_f1_df.loc[n_folds] = [\"avg\", np.mean(naive_bayes_f1), np.mean(knn_f1), np.mean(decision_tree_f1)]\n",
    "# Add 12th row in fold column as std\n",
    "models_f1_df.loc[n_folds+1] = [\"std\", np.std(naive_bayes_f1), np.std(knn_f1), np.std(decision_tree_f1)]\n",
    "\n",
    "# Make into latex table using the to_latex function\n",
    "models_f1_print_latex = models_f1_df.to_latex(index=False, header=True, float_format=\"%.3f\", column_format=\"|l|c|c|c|\", escape=False)\n",
    "\n",
    "# Adjust LaTeX to add horizontal lines and match desired style\n",
    "models_f1_print_latex = models_f1_print_latex.replace(\"\\\\toprule\", \"\\\\hline\")\n",
    "models_f1_print_latex = models_f1_print_latex.replace(\"\\\\bottomrule\", \"\\\\hline\")\n",
    "\n",
    "\n",
    "# --- For time ---\n",
    "\n",
    "# Make new df for time\n",
    "models_time_df = pd.DataFrame(columns=[\"Fold\", \"Naive Bayes\", \"KNN\", \"Decision Tree\"])\n",
    "models_time_df[\"Fold\"] = range(1, n_folds+1)\n",
    "models_time_df[\"Naive Bayes\"] = naive_bates_time\n",
    "models_time_df[\"KNN\"] = knn_time\n",
    "models_time_df[\"Decision Tree\"] = decision_tree_time\n",
    "\n",
    "# Add 11th row in fold column as mean\n",
    "models_time_df.loc[n_folds] = [\"avg\", np.mean(naive_bates_time), np.mean(knn_time), np.mean(decision_tree_time)]\n",
    "# Add 12th row in fold column as std\n",
    "models_time_df.loc[n_folds+1] = [\"std\", np.std(naive_bates_time), np.std(knn_time), np.std(decision_tree_time)]\n",
    "\n",
    "# Make into latex table using the to_latex function\n",
    "models_time_print_latex = models_time_df.to_latex(index=False, header=True, float_format=\"%.3f\", column_format=\"|l|c|c|c|\", escape=False)\n",
    "\n",
    "# Adjust LaTeX to add horizontal lines and match desired style\n",
    "models_time_print_latex = models_time_print_latex.replace(\"\\\\toprule\", \"\\\\hline\")\n",
    "models_time_print_latex = models_time_print_latex.replace(\"\\\\bottomrule\", \"\\\\hline\")\n",
    "\n",
    "\n",
    "# --- Print the latex tables ---\n",
    "print(\"Accuracy latex table:\")\n",
    "print(models_accuracy_print_latex)\n",
    "print(\"\\n\")\n",
    "print(\"F1 latex table:\")\n",
    "print(models_f1_print_latex)\n",
    "print(\"\\n\")\n",
    "print(\"Time latex table:\")\n",
    "print(models_time_print_latex)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct friedman test for each performance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg rank for accuracy: 2.0\n",
      "Sum of squared differences for accuracy: 45.2\n",
      "Second sum of squared differences for accuracy: 2.5\n",
      "Friedman statistic for accuracy: 18.080000000000002\n",
      "Critical value for accuracy: 5.991464547107979\n",
      "Reject null hypothesis for friedman test (significant difference between models)\n"
     ]
    }
   ],
   "source": [
    "# --- Ranking stuff and latex table stuff ---\n",
    "# Create ranking dataframe for the table\n",
    "latex_rankings_df = pd.DataFrame({\"Data Set\": range(1, n_folds + 1), \"Naive Bayes\": naive_bayes_accuracy, \"KNN\": knn_accuracy, \"Decision Tree\": decision_tree_accuracy})\n",
    "rankings_per_model = {\"Naive Bayes\": [], \"KNN\": [], \"Decision Tree\": []}\n",
    "\n",
    "# Loop through all models and rank each row based on accuracy and put the ranking as a paranthesis next to the accuracy\n",
    "for i, row in latex_rankings_df.iterrows():\n",
    "    # Rank the current row\n",
    "    rankings_current_row = row.rank(ascending=False, method=\"min\").astype(float)\n",
    "    # Insert paranthesis next to the accuracy in the dataframe\n",
    "    for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "        rankings_per_model[model].append(rankings_current_row[model])\n",
    "        # Change format of dataframe i, model to be in string format to accomodate paranthesis\n",
    "        latex_rankings_df[model] = latex_rankings_df[model].astype(str)\n",
    "        # Add paranthesis to the value\n",
    "        latex_rankings_df.at[i, model] = f\"{row[model]:.3f} ({int(rankings_current_row[model] - 1)})\"\n",
    "\n",
    "# Add mean and std rows to latex table\n",
    "latex_rankings_df.loc[n_folds] = [\"avg\", f\"{np.mean(naive_bayes_accuracy):.3f}\", f\"{np.mean(knn_accuracy):.3f}\", f\"{np.mean(decision_tree_accuracy):.3f}\"]\n",
    "# Add std row to latex table\n",
    "latex_rankings_df.loc[n_folds+1] = [\"std\", f\"{np.std(naive_bayes_accuracy):.3f}\", f\"{np.std(knn_accuracy):.3f}\", f\"{np.std(decision_tree_accuracy):.3f}\"]\n",
    "# Make into latex table using the to_latex function\n",
    "accuracy_rankings_latex = latex_rankings_df.to_latex(index=False, header=True, float_format=\"%.4f\", column_format=\"|l|c|c|c|\", escape=False)\n",
    "# Adjust LaTeX to add horizontal lines and match desired style\n",
    "accuracy_rankings_latex = accuracy_rankings_latex.replace(\"\\\\toprule\", \"\\\\hline\")\n",
    "accuracy_rankings_latex = accuracy_rankings_latex.replace(\"\\\\bottomrule\", \"\\\\hline\")\n",
    "\n",
    "# --- Actual friedmann test stuff ---\n",
    "# Vars for the friedman test\n",
    "num_samples = n_folds\n",
    "num_models = 3\n",
    "alpha = 0.05\n",
    "degrees_of_freedom = num_samples - 1\n",
    "\n",
    "# Calculate avg rank\n",
    "avg_rank_friedmann = (num_models + 1) / 2\n",
    "print(f\"Avg rank for accuracy: {avg_rank_friedmann}\")\n",
    "\n",
    "# Calculate first sum of squared differences as per the formula in book\n",
    "sum_squared_diff = 0\n",
    "for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "    Rj = sum(rankings_per_model[model]) / num_samples\n",
    "    sum_squared_diff += (Rj - avg_rank_friedmann)**2\n",
    "sum_squared_diff *= num_samples\n",
    "print(f\"Sum of squared differences for accuracy: {sum_squared_diff}\")\n",
    "\n",
    "# Calculate second sum of squared differences as per the formula in book\n",
    "second_sum_squared_diff = 0\n",
    "for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "    for i in range(num_samples):\n",
    "        second_sum_squared_diff += (rankings_per_model[model][i] - avg_rank_friedmann)**2\n",
    "second_sum_squared_diff *= (1 / (num_samples * (num_models - 1)))\n",
    "print(f\"Second sum of squared differences for accuracy: {second_sum_squared_diff}\")\n",
    "\n",
    "# Calculate friedman statistic\n",
    "friedman_statistic = sum_squared_diff / second_sum_squared_diff\n",
    "print(f\"Friedman statistic for accuracy: {friedman_statistic}\")\n",
    "\n",
    "# Calculate critical value\n",
    "critical_value = stats.chi2.ppf(1 - alpha, num_models - 1)\n",
    "print(f\"Critical value for accuracy: {critical_value}\")\n",
    "\n",
    "# Check if we reject the null hypothesis\n",
    "if friedman_statistic > critical_value:\n",
    "    print(\"Reject null hypothesis for friedman test (significant difference between models)\")\n",
    "else:\n",
    "    print(\"Do not reject null hypothesis for friedman test (no significant difference between models)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg rank f1: 2.0\n",
      "Sum of squared differences f1: 50.0\n",
      "Second sum of squared differences f1: 2.5\n",
      "Friedman statistic f1: 20.0\n",
      "Critical value f1: 5.991464547107979\n",
      "Reject null hypothesis for f1 (significant difference between models)\n"
     ]
    }
   ],
   "source": [
    "# --- Latex table for f1 scores ---\n",
    "# Create ranking dataframe for the table\n",
    "latex_rankings_df_f1 = pd.DataFrame({\"Data Set\": range(1, n_folds + 1), \"Naive Bayes\": naive_bayes_f1, \"KNN\": knn_f1, \"Decision Tree\": decision_tree_f1})\n",
    "rankings_per_model_f1 = {\"Naive Bayes\": [], \"KNN\": [], \"Decision Tree\": []}\n",
    "\n",
    "# Loop through all models and rank each row based on accuracy and put the ranking as a paranthesis next to the accuracy\n",
    "for i, row in latex_rankings_df_f1.iterrows():\n",
    "    # Rank the current row\n",
    "    rankings_current_row = row.rank(ascending=False, method=\"min\").astype(float)\n",
    "    # Insert paranthesis next to the accuracy in the dataframe\n",
    "    for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "        rankings_per_model_f1[model].append(rankings_current_row[model])\n",
    "        # Change format of dataframe i, model to be in string format to accomodate paranthesis\n",
    "        latex_rankings_df_f1[model] = latex_rankings_df_f1[model].astype(str)\n",
    "        # Add paranthesis to the value\n",
    "        latex_rankings_df_f1.at[i, model] = f\"{row[model]:.3f} ({int(rankings_current_row[model] - 1)})\"\n",
    "\n",
    "# Add mean and std rows to latex table\n",
    "latex_rankings_df_f1.loc[n_folds] = [\"avg\", f\"{np.mean(naive_bayes_f1):.3f}\", f\"{np.mean(knn_f1):.3f}\", f\"{np.mean(decision_tree_f1):.3f}\"]\n",
    "# Add std row to latex table\n",
    "latex_rankings_df_f1.loc[n_folds+1] = [\"std\", f\"{np.std(naive_bayes_f1):.3f}\", f\"{np.std(knn_f1):.3f}\", f\"{np.std(decision_tree_f1):.3f}\"]\n",
    "# Make into latex table using the to_latex function\n",
    "f1_rankings_latex = latex_rankings_df_f1.to_latex(index=False, header=True, float_format=\"%.4f\", column_format=\"|l|c|c|c|\", escape=False)\n",
    "# Adjust LaTeX to add horizontal lines and match desired style\n",
    "f1_rankings_latex = f1_rankings_latex.replace(\"\\\\toprule\", \"\\\\hline\")\n",
    "f1_rankings_latex = f1_rankings_latex.replace(\"\\\\bottomrule\", \"\\\\hline\")\n",
    "\n",
    "# --- Actual friedmann test stuff ---\n",
    "# Vars for the friedman test\n",
    "num_samples = n_folds\n",
    "num_models = 3\n",
    "alpha = 0.05\n",
    "\n",
    "# Calculate avg rank\n",
    "avg_rank_friedmann = (num_models + 1) / 2\n",
    "print(f\"Avg rank f1: {avg_rank_friedmann}\")\n",
    "\n",
    "# Calculate first sum of squared differences as per the formula in book\n",
    "sum_squared_diff = 0\n",
    "for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "    Rj = sum(rankings_per_model_f1[model]) / num_samples\n",
    "    sum_squared_diff += (Rj - avg_rank_friedmann)**2\n",
    "sum_squared_diff *= num_samples\n",
    "print(f\"Sum of squared differences f1: {sum_squared_diff}\")\n",
    "\n",
    "# Calculate second sum of squared differences as per the formula in book\n",
    "second_sum_squared_diff = 0\n",
    "for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "    for i in range(num_samples):\n",
    "        second_sum_squared_diff += (rankings_per_model_f1[model][i] - avg_rank_friedmann)**2\n",
    "second_sum_squared_diff *= (1 / (num_samples * (num_models - 1)))\n",
    "print(f\"Second sum of squared differences f1: {second_sum_squared_diff}\")\n",
    "\n",
    "# Calculate friedman statistic\n",
    "friedman_statistic = sum_squared_diff / second_sum_squared_diff\n",
    "print(f\"Friedman statistic f1: {friedman_statistic}\")\n",
    "\n",
    "# Calculate critical value\n",
    "critical_value = stats.chi2.ppf(1 - alpha, num_models - 1)\n",
    "print(f\"Critical value f1: {critical_value}\")\n",
    "\n",
    "# Check if we reject the null hypothesis\n",
    "if friedman_statistic > critical_value:\n",
    "    print(\"Reject null hypothesis for f1 (significant difference between models)\")\n",
    "else:\n",
    "    print(\"Do not reject null hypothesis for f1 (no significant difference between models)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg rank for time: 2.0\n",
      "Sum of squared differences for time: 20.0\n",
      "Second sum of squared differences for time: 1.0\n",
      "Friedman statistic for time: 20.0\n",
      "Critical value for time: 5.991464547107979\n",
      "Reject null hypothesis for friedman test (significant difference between models)\n"
     ]
    }
   ],
   "source": [
    "# --- Latex table for time ---\n",
    "# Create ranking dataframe for the table\n",
    "latex_rankings_df_time = pd.DataFrame({\"Data Set\": range(1, n_folds + 1), \"Naive Bayes\": naive_bates_time, \"KNN\": knn_time, \"Decision Tree\": decision_tree_time})\n",
    "rankings_per_model_time = {\"Naive Bayes\": [], \"KNN\": [], \"Decision Tree\": []}\n",
    "\n",
    "# Loop through all models and rank each row based on accuracy and put the ranking as a paranthesis next to the accuracy\n",
    "for i, row in latex_rankings_df_time.iterrows():\n",
    "    # Rank the current row\n",
    "    rankings_current_row = row.rank(ascending=True, method=\"min\").astype(float)\n",
    "    # Insert paranthesis next to the accuracy in the dataframe\n",
    "    for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "        rankings_per_model_time[model].append(rankings_current_row[model])\n",
    "        # Change format of dataframe i, model to be in string format to accomodate paranthesis\n",
    "        latex_rankings_df_time[model] = latex_rankings_df_time[model].astype(str)\n",
    "        # Add the paranthesis to the dataframe\n",
    "        latex_rankings_df_time.at[i, model] = f\"{row[model]:.3f} ({int(rankings_current_row[model])})\"\n",
    "\n",
    "# Add mean and std rows to latex table\n",
    "latex_rankings_df_time.loc[n_folds] = [\"avg\", f\"{np.mean(naive_bates_time):.3f}\", f\"{np.mean(knn_time):.3f}\", f\"{np.mean(decision_tree_time):.3f}\"]\n",
    "# Add std row to latex table\n",
    "latex_rankings_df_time.loc[n_folds+1] = [\"std\", f\"{np.std(naive_bates_time):.3f}\", f\"{np.std(knn_time):.3f}\", f\"{np.std(decision_tree_time):.3f}\"]\n",
    "# Make into latex table using the to_latex function\n",
    "time_rankings_latex = latex_rankings_df_time.to_latex(index=False, header=True, float_format=\"%.4f\", column_format=\"|l|c|c|c|\", escape=False)\n",
    "# Adjust LaTeX to add horizontal lines and match desired style\n",
    "time_rankings_latex = time_rankings_latex.replace(\"\\\\toprule\", \"\\\\hline\")\n",
    "time_rankings_latex = time_rankings_latex.replace(\"\\\\bottomrule\", \"\\\\hline\")\n",
    "\n",
    "# --- Actual friedmann test stuff ---\n",
    "# Vars for the friedman test\n",
    "num_samples = n_folds\n",
    "num_models = 3\n",
    "alpha = 0.05\n",
    "degrees_of_freedom = num_samples - 1\n",
    "\n",
    "# Calculate avg rank\n",
    "avg_rank_friedmann = (num_models + 1) / 2\n",
    "print(f\"Avg rank for time: {avg_rank_friedmann}\")\n",
    "\n",
    "# Calculate first sum of squared differences as per the formula in book\n",
    "sum_squared_diff = 0\n",
    "for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "    Rj = sum(rankings_per_model_time[model]) / num_samples\n",
    "    sum_squared_diff += (Rj - avg_rank_friedmann)**2\n",
    "sum_squared_diff *= num_samples\n",
    "print(f\"Sum of squared differences for time: {sum_squared_diff}\")\n",
    "\n",
    "# Calculate second sum of squared differences as per the formula in book\n",
    "second_sum_squared_diff = 0\n",
    "for model in [\"Naive Bayes\", \"KNN\", \"Decision Tree\"]:\n",
    "    for i in range(num_samples):\n",
    "        second_sum_squared_diff += (rankings_per_model_time[model][i] - avg_rank_friedmann)**2\n",
    "second_sum_squared_diff *= (1 / (num_samples * (num_models - 1)))\n",
    "print(f\"Second sum of squared differences for time: {second_sum_squared_diff}\")\n",
    "\n",
    "# Calculate friedman statistic\n",
    "friedman_statistic = sum_squared_diff / second_sum_squared_diff\n",
    "print(f\"Friedman statistic for time: {friedman_statistic}\")\n",
    "\n",
    "# Calculate critical value\n",
    "critical_value = stats.chi2.ppf(1 - alpha, num_models - 1)\n",
    "print(f\"Critical value for time: {critical_value}\")\n",
    "\n",
    "# Check if we reject the null hypothesis\n",
    "if friedman_statistic > critical_value:\n",
    "    print(\"Reject null hypothesis for friedman test (significant difference between models)\")\n",
    "else:\n",
    "    print(\"Do not reject null hypothesis for friedman test (no significant difference between models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables as in example 12.8 in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rankings latex table:\n",
      "\\begin{tabular}{|l|c|c|c|}\n",
      "\\hline\n",
      "Data Set & Naive Bayes & KNN & Decision Tree \\\\\n",
      "\\midrule\n",
      "1 & 0.826 (2) & 0.807 (3) & 0.909 (1) \\\\\n",
      "2 & 0.841 (2) & 0.787 (3) & 0.889 (1) \\\\\n",
      "3 & 0.826 (2) & 0.793 (3) & 0.917 (1) \\\\\n",
      "4 & 0.811 (3) & 0.830 (2) & 0.911 (1) \\\\\n",
      "5 & 0.846 (2) & 0.813 (3) & 0.911 (1) \\\\\n",
      "6 & 0.826 (3) & 0.848 (2) & 0.926 (1) \\\\\n",
      "7 & 0.839 (2) & 0.822 (3) & 0.924 (1) \\\\\n",
      "8 & 0.780 (3) & 0.800 (2) & 0.915 (1) \\\\\n",
      "9 & 0.780 (3) & 0.796 (2) & 0.893 (1) \\\\\n",
      "10 & 0.826 (2) & 0.787 (3) & 0.911 (1) \\\\\n",
      "avg & 0.820 & 0.808 & 0.911 \\\\\n",
      "std & 0.022 & 0.019 & 0.011 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "F1 rankings latex table:\n",
      "\\begin{tabular}{|l|c|c|c|}\n",
      "\\hline\n",
      "Data Set & Naive Bayes & KNN & Decision Tree \\\\\n",
      "\\midrule\n",
      "1 & 0.810 (2) & 0.753 (3) & 0.888 (1) \\\\\n",
      "2 & 0.827 (2) & 0.732 (3) & 0.861 (1) \\\\\n",
      "3 & 0.815 (2) & 0.740 (3) & 0.895 (1) \\\\\n",
      "4 & 0.800 (2) & 0.780 (3) & 0.887 (1) \\\\\n",
      "5 & 0.830 (2) & 0.758 (3) & 0.885 (1) \\\\\n",
      "6 & 0.814 (2) & 0.800 (3) & 0.908 (1) \\\\\n",
      "7 & 0.826 (2) & 0.776 (3) & 0.902 (1) \\\\\n",
      "8 & 0.769 (2) & 0.753 (3) & 0.896 (1) \\\\\n",
      "9 & 0.775 (2) & 0.749 (3) & 0.869 (1) \\\\\n",
      "10 & 0.813 (2) & 0.718 (3) & 0.887 (1) \\\\\n",
      "avg & 0.808 & 0.756 & 0.888 \\\\\n",
      "std & 0.020 & 0.023 & 0.014 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "Time rankings latex table:\n",
      "\\begin{tabular}{|l|c|c|c|}\n",
      "\\hline\n",
      "Data Set & Naive Bayes & KNN & Decision Tree \\\\\n",
      "\\midrule\n",
      "1 & 0.007 (2) & 0.006 (1) & 0.079 (3) \\\\\n",
      "2 & 0.007 (2) & 0.006 (1) & 0.062 (3) \\\\\n",
      "3 & 0.008 (2) & 0.005 (1) & 0.069 (3) \\\\\n",
      "4 & 0.008 (2) & 0.004 (1) & 0.067 (3) \\\\\n",
      "5 & 0.008 (2) & 0.005 (1) & 0.062 (3) \\\\\n",
      "6 & 0.010 (2) & 0.005 (1) & 0.062 (3) \\\\\n",
      "7 & 0.006 (2) & 0.005 (1) & 0.054 (3) \\\\\n",
      "8 & 0.008 (2) & 0.006 (1) & 0.072 (3) \\\\\n",
      "9 & 0.008 (2) & 0.005 (1) & 0.067 (3) \\\\\n",
      "10 & 0.008 (2) & 0.005 (1) & 0.071 (3) \\\\\n",
      "avg & 0.008 & 0.005 & 0.066 \\\\\n",
      "std & 0.001 & 0.001 & 0.007 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Print the latex tables ---\n",
    "print(\"Accuracy rankings latex table:\")\n",
    "print(accuracy_rankings_latex)\n",
    "print(\"\\n\")\n",
    "print(\"F1 rankings latex table:\")\n",
    "print(f1_rankings_latex)\n",
    "print(\"\\n\")\n",
    "print(\"Time rankings latex table:\")\n",
    "print(time_rankings_latex)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine whether the average ranks as a whole display significant differences on the 0.05 alpha level and, if so, use the Nemenyi test to calculate the critical difference in order to determine which algorithms perform significantly different from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical difference: 1.0478214542564015\n",
      "Nemenyi test for accuracy:\n",
      "Naive Bayes and KNN for accuracy are not significantly different since the difference is 0.20000000000000018 which is less than the critical difference of 1.0478214542564015\n",
      "Naive Bayes and Decision Tree for accuracy are significantly different since the difference is 1.4 which is greater than the critical difference of 1.0478214542564015\n",
      "KNN and Decision Tree for accuracy are significantly different since the difference is 1.6 which is greater than the critical difference of 1.0478214542564015\n",
      "\n",
      "\n",
      "Nemenyi test for f1:\n",
      "Naive Bayes and KNN for f1 are not significantly different since the difference is 1.0 which is less than the critical difference of 1.0478214542564015\n",
      "Naive Bayes and Decision Tree for f1 are not significantly different since the difference is 1.0 which is less than the critical difference of 1.0478214542564015\n",
      "KNN and Decision Tree for f1 are significantly different since the difference is 2.0 which is greater than the critical difference of 1.0478214542564015\n",
      "\n",
      "\n",
      "Nemenyi test for time:\n",
      "Naive Bayes and KNN for time are not significantly different since the difference is 1.0 which is less than the critical difference of 1.0478214542564015\n",
      "Naive Bayes and Decision Tree for time are not significantly different since the difference is 1.0 which is less than the critical difference of 1.0478214542564015\n",
      "KNN and Decision Tree for time are significantly different since the difference is 2.0 which is greater than the critical difference of 1.0478214542564015\n"
     ]
    }
   ],
   "source": [
    "# Determine whether the average ranks as a whole display significant differences on the 0.05 alpha level and, \n",
    "# if so, use the Nemenyi test to calculate the critical difference in order to determine which algorithms perform significantly different from each other.\n",
    "\n",
    "# --- Nemenyi test stuff ---\n",
    "# Calculate critical difference\n",
    "q_alpha = 2.343 # According to course book page 356 since we have alpha = 0.05 and k = 3 (models). This does not take degrees of freedom into account\n",
    "\n",
    "# Calculate critical difference\n",
    "critical_difference = q_alpha * np.sqrt((num_models * (num_models + 1)) / (6 * num_samples))\n",
    "print(f\"Critical difference: {critical_difference}\")\n",
    "\n",
    "\n",
    "# --- Nemenyi test for accuracy ---\n",
    "print(\"Nemenyi test for accuracy:\")\n",
    "# Loop through all models and compare them with each other\n",
    "for i, model1 in enumerate([\"Naive Bayes\", \"KNN\", \"Decision Tree\"]):\n",
    "    for j, model2 in enumerate([\"Naive Bayes\", \"KNN\", \"Decision Tree\"]):\n",
    "        # If i is less than j, then we have not compared the models yet\n",
    "        if i < j:\n",
    "            # Calculate the difference between the models\n",
    "            difference = abs(np.mean(rankings_per_model[model1]) - np.mean(rankings_per_model[model2]))\n",
    "            # Check if the difference is significant\n",
    "            if difference > critical_difference:\n",
    "                print(f\"{model1} and {model2} for accuracy are significantly different since the difference is {difference} which is greater than the critical difference of {critical_difference}\")\n",
    "            else:\n",
    "                print(f\"{model1} and {model2} for accuracy are not significantly different since the difference is {difference} which is less than the critical difference of {critical_difference}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Nemenyi test for f1 ---\n",
    "print(\"Nemenyi test for f1:\")\n",
    "# Loop through all models and compare them with each other\n",
    "for i, model1 in enumerate([\"Naive Bayes\", \"KNN\", \"Decision Tree\"]):\n",
    "    for j, model2 in enumerate([\"Naive Bayes\", \"KNN\", \"Decision Tree\"]):\n",
    "        # If i is less than j, then we have not compared the models yet\n",
    "        if i < j:\n",
    "            # Calculate the difference between the models\n",
    "            difference = abs(np.mean(rankings_per_model_f1[model1]) - np.mean(rankings_per_model_f1[model2]))\n",
    "            # Check if the difference is significant\n",
    "            if difference > critical_difference:\n",
    "                print(f\"{model1} and {model2} for f1 are significantly different since the difference is {difference} which is greater than the critical difference of {critical_difference}\")\n",
    "            else:\n",
    "                print(f\"{model1} and {model2} for f1 are not significantly different since the difference is {difference} which is less than the critical difference of {critical_difference}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Nemenyi test for time ---\n",
    "print(\"Nemenyi test for time:\")\n",
    "# Loop through all models and compare them with each other\n",
    "for i, model1 in enumerate([\"Naive Bayes\", \"KNN\", \"Decision Tree\"]):\n",
    "    for j, model2 in enumerate([\"Naive Bayes\", \"KNN\", \"Decision Tree\"]):\n",
    "        # If i is less than j, then we have not compared the models yet\n",
    "        if i < j:\n",
    "            # Calculate the difference between the models\n",
    "            difference = abs(np.mean(rankings_per_model_time[model1]) - np.mean(rankings_per_model_time[model2]))\n",
    "            # Check if the difference is significant\n",
    "            if difference > critical_difference:\n",
    "                print(f\"{model1} and {model2} for time are significantly different since the difference is {difference} which is greater than the critical difference of {critical_difference}\")\n",
    "            else:\n",
    "                print(f\"{model1} and {model2} for time are not significantly different since the difference is {difference} which is less than the critical difference of {critical_difference}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
